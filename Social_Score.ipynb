{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1vQvXEyoBzvs",
        "DW9_QmTxCXD1",
        "K-Msww98C519"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Etape 1**"
      ],
      "metadata": {
        "id": "1vQvXEyoBzvs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__cYri_S-cpo",
        "outputId": "6b395405-9d16-4e45-ffbb-625a65d7838b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier chargé: /content/train.csv\n",
            "Lignes utilisées pour tout le notebook: 3000 (N_ROWS=3000)\n",
            "\n",
            "Aperçu (id, comment_text) — 5 premières lignes :\n",
            "              id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       comment_text\n",
            "0000997932d777bf                                                                                                                                                                                                                                                                                                                                                                          Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
            "000103f0d9cfb60f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
            "000113f07ec002fd                                                                                                                                                                                                                                                                                                                                                                                                          Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
            "0001b41b1c6bb37e \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\n",
            "0001d958c54c6e35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                You, sir, are my hero. Any chance you remember what page that's on?\n"
          ]
        }
      ],
      "source": [
        "# === Bloc 0 — Config & lecture unique CSV ===\n",
        "# Chemin et découpe globale (tu peux changer N_ROWS ici, pour toutes les étapes)\n",
        "CSV_PATH = \"/content/train.csv\"\n",
        "N_ROWS   = 3000  # <- règle ici le nombre de lignes utilisées dans tout le notebook\n",
        "\n",
        "# Install (Colab) — versions raisonnables et compatibles\n",
        "# !pip -q install -U spacy tqdm scikit-learn tensorflow==2.16.1\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Modèle spaCy EN (souvent adapté aux datasets toxiques Kaggle)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Lecture unique du CSV\n",
        "df_all = pd.read_csv(CSV_PATH, dtype=str, keep_default_na=False)\n",
        "dfN = df_all.head(N_ROWS).copy()\n",
        "\n",
        "print(f\"Fichier chargé: {CSV_PATH}\")\n",
        "print(f\"Lignes utilisées pour tout le notebook: {len(dfN)} (N_ROWS={N_ROWS})\")\n",
        "print(\"\\nAperçu (id, comment_text) — 5 premières lignes :\")\n",
        "print(dfN.loc[:, [\"id\",\"comment_text\"]].head(5).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 1 — Params & Regex (DCP) ===\n",
        "USE_LABEL_TOKENS = True  # True => PERSON/EMAIL/... ; False => ****\n",
        "\n",
        "REGEX_PATTERNS = {\n",
        "    \"EMAIL\": re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"),\n",
        "    \"IP\":    re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\"),\n",
        "    \"URL\":   re.compile(r\"https?://\\S+|www\\.\\S+\"),\n",
        "    \"PHONE\": re.compile(r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{3,4}[-.\\s]?\\d{3,4}\\b\"),\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "zrfkVinb_kAk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 2 — Fonction d’anonymisation (DCP uniquement) ===\n",
        "# Ne masque via spaCy que PERSON (pas ORG/GPE...)\n",
        "\n",
        "# Compléments regex utiles (adresse postale, CB, @handle)\n",
        "ADDRESS_RE = re.compile(\n",
        "    r\"\\b\\d{1,5}\\s+(?:rue|avenue|av\\.?|bd|boulevard|impasse|allée|route|chemin|che\\.?|place|pl\\.?|quai|square|sq\\.?|\"\n",
        "    r\"street|st\\.?|ave\\.?|road|rd\\.?|blvd\\.?|lane|ln\\.?|drive|dr\\.?|court|ct\\.?)\\s+[A-Za-zÀ-ÖØ-öø-ÿ'’\\-\\. ]+\\b\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "CREDIT_CARD_RE = re.compile(r\"\\b(?:\\d[ -]*?){13,19}\\b\")\n",
        "USERNAME_RE    = re.compile(r\"(?<!\\w)@[\\w._\\-]{2,32}\")\n",
        "\n",
        "PII_LABELS = [\"EMAIL\",\"IP\",\"PHONE\",\"ADDRESS\",\"CREDIT_CARD\",\"USERNAME\",\"PERSON\"]\n",
        "\n",
        "def _mask(label: str) -> str:\n",
        "    return label if USE_LABEL_TOKENS else \"****\"\n",
        "\n",
        "def anonymize_text(text: str, counts: dict) -> str:\n",
        "    if not text:\n",
        "        return text\n",
        "    tmp = text\n",
        "\n",
        "    # 1) Regex DCP\n",
        "    for label in [\"EMAIL\",\"IP\",\"PHONE\"]:\n",
        "        rx = REGEX_PATTERNS[label]\n",
        "        def repl(m, L=label):\n",
        "            counts[L] = counts.get(L, 0) + 1\n",
        "            return _mask(L)\n",
        "        tmp = rx.sub(repl, tmp)\n",
        "\n",
        "    # Adresse\n",
        "    def addr_repl(m):\n",
        "        counts[\"ADDRESS\"] = counts.get(\"ADDRESS\", 0) + 1\n",
        "        return _mask(\"ADDRESS\")\n",
        "    tmp = ADDRESS_RE.sub(addr_repl, tmp)\n",
        "\n",
        "    # CB avec filtre minimal\n",
        "    def cc_repl(m):\n",
        "        digits = re.sub(r\"\\D\", \"\", m.group(0))\n",
        "        if 13 <= len(digits) <= 19:\n",
        "            counts[\"CREDIT_CARD\"] = counts.get(\"CREDIT_CARD\", 0) + 1\n",
        "            return _mask(\"CREDIT_CARD\")\n",
        "        return m.group(0)\n",
        "    tmp = CREDIT_CARD_RE.sub(cc_repl, tmp)\n",
        "\n",
        "    # @username\n",
        "    def user_repl(m):\n",
        "        counts[\"USERNAME\"] = counts.get(\"USERNAME\", 0) + 1\n",
        "        return _mask(\"USERNAME\")\n",
        "    tmp = USERNAME_RE.sub(user_repl, tmp)\n",
        "\n",
        "    # 2) spaCy NER — PERSON uniquement\n",
        "    doc = nlp(tmp)\n",
        "    out, last = [], 0\n",
        "    for ent in doc.ents:\n",
        "        if ent.start_char > last:\n",
        "            out.append(tmp[last:ent.start_char])\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            counts[\"PERSON\"] = counts.get(\"PERSON\", 0) + 1\n",
        "            out.append(_mask(\"PERSON\"))\n",
        "            last = ent.end_char\n",
        "    out.append(tmp[last:])\n",
        "    anonymized = \"\".join(out)\n",
        "\n",
        "    # Nettoyage: éviter PERSON PERSON consécutifs\n",
        "    anonymized = re.sub(r\"(PERSON)(?:\\s*,?\\s*PERSON)+\", \"PERSON\", anonymized)\n",
        "    return anonymized\n"
      ],
      "metadata": {
        "id": "YWuGgLeT_7Lc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 3 — Application & stats DCP ===\n",
        "assert \"dfN\" in globals(), \"dfN absent. Exécute d'abord le Bloc 0 (lecture CSV).\"\n",
        "\n",
        "counts_global = {}\n",
        "anon_list = []\n",
        "\n",
        "for _, row in tqdm(dfN.iterrows(), total=len(dfN)):\n",
        "    local = {}\n",
        "    anon_list.append(anonymize_text(row.get(\"comment_text\",\"\"), local))\n",
        "    for k, v in local.items():\n",
        "        counts_global[k] = counts_global.get(k, 0) + v\n",
        "\n",
        "dfN[\"comment_text_anonymized\"] = anon_list\n",
        "\n",
        "print(\"\\nAperçu anonymisé (5 premières lignes) :\")\n",
        "print(dfN.loc[:, [\"id\",\"comment_text_anonymized\"]].head(5).to_string(index=False))\n",
        "\n",
        "print(\"\\nComptage global DCP :\")\n",
        "for k in sorted([k for k in counts_global if k in PII_LABELS]):\n",
        "    print(f\"  - {k:>11}: {counts_global[k]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx7sj_fyAABk",
        "outputId": "8b21b46d-b6e7-4f75-9fe0-721a9791a547"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [01:05<00:00, 45.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aperçu anonymisé (5 premières lignes) :\n",
            "              id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   comment_text_anonymized\n",
            "0000997932d777bf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Explanation\\nWhy the edits made under my username Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.IP\n",
            "000103f0d9cfb60f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\n",
            "000113f07ec002fd                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
            "0001b41b1c6bb37e \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"\n",
            "0001d958c54c6e35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       You, sir, are my hero. Any chance you remember what page that's on?\n",
            "\n",
            "Comptage global DCP :\n",
            "  - CREDIT_CARD: 1\n",
            "  -       EMAIL: 9\n",
            "  -          IP: 206\n",
            "  -      PERSON: 2254\n",
            "  -       PHONE: 55\n",
            "  -    USERNAME: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 4 — Comparaison (tronquée pour lisibilité) ===\n",
        "def short(s, n=140):\n",
        "    s = str(s).replace(\"\\n\", \" \")\n",
        "    return (s[:n] + \"…\") if len(s) > n else s\n",
        "\n",
        "preview_cols = [\"id\", \"comment_text\", \"comment_text_anonymized\"]\n",
        "preview = dfN.loc[:, preview_cols].copy()\n",
        "preview[\"comment_text\"] = preview[\"comment_text\"].apply(lambda x: short(x, 120))\n",
        "preview[\"comment_text_anonymized\"] = preview[\"comment_text_anonymized\"].apply(lambda x: short(x, 120))\n",
        "\n",
        "print(\"Original ↔ Anonymisé (10 premières lignes) :\")\n",
        "print(preview.head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6elVws1Bb9x",
        "outputId": "19d0c34a-357c-4b97-b66d-73b5703d52d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original ↔ Anonymisé (10 premières lignes) :\n",
            "              id                                                                                                              comment_text                                                                                                   comment_text_anonymized\n",
            "0000997932d777bf Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just clo… Explanation Why the edits made under my username Explanation Why the edits made under my username Hardcore Metallica Fan…\n",
            "000103f0d9cfb60f          D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC) D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) D'aww! He matches this background col…\n",
            "000113f07ec002fd Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talk… Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talk…\n",
            "0001b41b1c6bb37e \" More I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a … \" More I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a …\n",
            "0001d958c54c6e35                                                       You, sir, are my hero. Any chance you remember what page that's on?                                                       You, sir, are my hero. Any chance you remember what page that's on?\n",
            "00025465d4725e87                                                         \"  Congratulations from me as well, use the tools well.  · talk \"                                                         \"  Congratulations from me as well, use the tools well.  · talk \"\n",
            "0002bcb3da6cb337                                                                              COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK                                                                              COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
            "00031b1e95af7921       Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.                 Your vandalism to the PERSON article has been reverted.  Please don't do it again, or you will be banned.\n",
            "00037261f536c51d Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they w… Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they w…\n",
            "00040093b2687caa                                                    alignment on this subject and which are contrary to those of DuLithgow                                                       alignment on this subject and which are contrary to those of PERSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 5 — Registre (imprimé) ===\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"\\n=== REGISTRE (imprimé) — Anonymisation DCP uniquement ===\")\n",
        "print(f\"Date UTC         : {datetime.utcnow().isoformat()}Z\")\n",
        "print(f\"Fichier traité   : {CSV_PATH} (N_ROWS={len(dfN)})\")\n",
        "print(\"Modèle NER       : spaCy en_core_web_sm (PERSON uniquement)\")\n",
        "print(\"Catégories DCP   : PERSON, EMAIL, PHONE, ADDRESS, IP, USERNAME, CREDIT_CARD\")\n",
        "print(f\"Sortie/format    : {'Labels (PERSON/EMAIL/...)' if USE_LABEL_TOKENS else '**** (masquage intégral)'}\")\n",
        "\n",
        "print(\"\\nComptage DCP :\")\n",
        "for k in [\"PERSON\",\"EMAIL\",\"PHONE\",\"ADDRESS\",\"IP\",\"USERNAME\",\"CREDIT_CARD\"]:\n",
        "    if k in counts_global:\n",
        "        print(f\"  - {k:>11}: {counts_global[k]}\")\n",
        "print(\"\\nNote : vérifier manuellement un échantillon — les adresses postales sont difficiles à capter parfaitement.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL-Lde5DBo-I",
        "outputId": "df8f8403-9733-48d2-d223-95f5401c7d53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== REGISTRE (imprimé) — Anonymisation DCP uniquement ===\n",
            "Date UTC         : 2025-11-06T13:35:58.016468Z\n",
            "Fichier traité   : /content/train.csv (N_ROWS=3000)\n",
            "Modèle NER       : spaCy en_core_web_sm (PERSON uniquement)\n",
            "Catégories DCP   : PERSON, EMAIL, PHONE, ADDRESS, IP, USERNAME, CREDIT_CARD\n",
            "Sortie/format    : Labels (PERSON/EMAIL/...)\n",
            "\n",
            "Comptage DCP :\n",
            "  -      PERSON: 2254\n",
            "  -       EMAIL: 9\n",
            "  -       PHONE: 55\n",
            "  -          IP: 206\n",
            "  -    USERNAME: 3\n",
            "  - CREDIT_CARD: 1\n",
            "\n",
            "Note : vérifier manuellement un échantillon — les adresses postales sont difficiles à capter parfaitement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3631366938.py:5: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  print(f\"Date UTC         : {datetime.utcnow().isoformat()}Z\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Etape 2**"
      ],
      "metadata": {
        "id": "DW9_QmTxCXD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 1 — Nettoyage + Y multilabel ===\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Colonne texte : si anonymisé existe, on l'utilise\n",
        "text_col = \"comment_text_anonymized\" if \"comment_text_anonymized\" in dfN.columns else \"comment_text\"\n",
        "\n",
        "label_cols = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
        "for c in label_cols:\n",
        "    dfN[c] = pd.to_numeric(dfN[c], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "EMOJI_RE = re.compile(r\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
        "URL_RE   = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "def clean_text(s: str) -> str:\n",
        "    s = str(s).lower()\n",
        "    s = URL_RE.sub(\" \", s)\n",
        "    s = EMOJI_RE.sub(\" \", s)\n",
        "    s = re.sub(r\"[^a-z0-9\\s']\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "dfN[\"text_clean\"] = dfN[text_col].apply(clean_text)\n",
        "X = dfN[\"text_clean\"].astype(str).tolist()\n",
        "Y = dfN[label_cols].values\n",
        "\n",
        "print(\"Aperçu (texte nettoyé) — 5 premières lignes :\")\n",
        "print(dfN.loc[:, [\"id\", text_col, \"text_clean\"]].head(5).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHgi1zBRCcIa",
        "outputId": "4c4e92fe-d0db-452c-a100-67787078ad78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aperçu (texte nettoyé) — 5 premières lignes :\n",
            "              id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   comment_text_anonymized                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text_clean\n",
            "0000997932d777bf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Explanation\\nWhy the edits made under my username Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.IP                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  explanation why the edits made under my username explanation why the edits made under my username hardcore metallica fan were reverted they weren't vandalisms just closure on some gas after i voted at explanation why the edits made under my username hardcore metallica fan were reverted they weren't vandalisms just closure on some gas after i voted at new york dolls explanation why the edits made under my username hardcore metallica fan were reverted they weren't vandalisms just closure on some gas after i voted at new york dolls fac and please don't remove the template from the talk page since i'm retired now explanation why the edits made under my username hardcore metallica fan were reverted they weren't vandalisms just closure on some gas after i voted at new york dolls fac and please don't remove the template from the talk page since i'm retired now ip\n",
            "000103f0d9cfb60f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                d'aww he matches this background colour i'm seemingly stuck with thanks talk d'aww he matches this background colour i'm seemingly stuck with thanks talk 21 51 january 11 2016 d'aww he matches this background colour i'm seemingly stuck with thanks talk 21 51 january 11 2016 utc\n",
            "000113f07ec002fd                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 hey man i'm really not trying to edit war it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info\n",
            "0001b41b1c6bb37e \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \" more i can't make any real suggestions on improvement i wondered if the section statistics should be later on or a subsection of types of accidents i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no one else does more i can't make any real suggestions on improvement i wondered if the section statistics should be later on or a subsection of types of accidents i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no one else does first if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it's listed in the relevant form eg more i can't make any real suggestions on improvement i wondered if the section statistics should be later on or a subsection of types of accidents i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no one else does first if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it's listed in the relevant form eg wikipedia more i can't make any real suggestions on improvement i wondered if the section statistics should be later on or a subsection of types of accidents i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no one else does first if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it's listed in the relevant form eg wikipedia good article nominations transport\n",
            "0001d958c54c6e35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       You, sir, are my hero. Any chance you remember what page that's on?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       you sir are my hero any chance you remember what page that's on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 2 — Split train/val ===\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Taille train/val :\", len(X_train), \"/\", len(X_val))\n",
        "print(\"\\nDistribution des labels (somme 1/0) — TRAIN :\")\n",
        "print(pd.Series(Y_train.sum(axis=0), index=label_cols).to_string())\n",
        "print(\"\\nDistribution des labels (somme 1/0) — VAL :\")\n",
        "print(pd.Series(Y_val.sum(axis=0), index=label_cols).to_string())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61Ksc6vpCi_b",
        "outputId": "e505d0de-6245-41a9-d3df-9c4d03ba4e22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille train/val : 2100 / 900\n",
            "\n",
            "Distribution des labels (somme 1/0) — TRAIN :\n",
            "toxic            198\n",
            "severe_toxic      25\n",
            "obscene          104\n",
            "threat            11\n",
            "insult           102\n",
            "identity_hate     17\n",
            "\n",
            "Distribution des labels (somme 1/0) — VAL :\n",
            "toxic            109\n",
            "severe_toxic      13\n",
            "obscene           63\n",
            "threat             3\n",
            "insult            62\n",
            "identity_hate     15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 3 — Modèle LSTM bi-directionnel (unique, pas de baseline) ===\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "MAX_VOCAB  = 8000\n",
        "MAX_LEN    = 120\n",
        "tokenizer  = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "Xtr_seq = tokenizer.texts_to_sequences(X_train)\n",
        "Xva_seq = tokenizer.texts_to_sequences(X_val)\n",
        "Xtr_pad = pad_sequences(Xtr_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "Xva_pad = pad_sequences(Xva_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=MAX_VOCAB, output_dim=64),\n",
        "    layers.Bidirectional(layers.LSTM(64)),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(len(label_cols), activation=\"sigmoid\")  # multilabel\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[])\n",
        "\n",
        "EPOCHS = 8\n",
        "BATCH  = 8\n",
        "t0 = time.perf_counter()\n",
        "hist = model.fit(\n",
        "    Xtr_pad, Y_train,\n",
        "    validation_data=(Xva_pad, Y_val),\n",
        "    epochs=EPOCHS, batch_size=BATCH, verbose=0\n",
        ")\n",
        "train_time_lstm = time.perf_counter() - t0\n",
        "\n",
        "t1 = time.perf_counter()\n",
        "Yp = model.predict(Xva_pad, verbose=0)\n",
        "pred_time_lstm = time.perf_counter() - t1\n",
        "Yhat = (Yp >= 0.5).astype(int)\n",
        "\n",
        "prec_micro_l = precision_score(Y_val, Yhat, average=\"micro\",  zero_division=0)\n",
        "rec_micro_l  = recall_score(Y_val,  Yhat, average=\"micro\",    zero_division=0)\n",
        "f1_micro_l   = f1_score(Y_val,      Yhat, average=\"micro\",    zero_division=0)\n",
        "prec_macro_l = precision_score(Y_val, Yhat, average=\"macro\",  zero_division=0)\n",
        "rec_macro_l  = recall_score(Y_val,  Yhat, average=\"macro\",    zero_division=0)\n",
        "f1_macro_l   = f1_score(Y_val,      Yhat, average=\"macro\",    zero_division=0)\n",
        "\n",
        "print(\"=== Modèle (BiLSTM) ===\")\n",
        "print(f\"Temps entraînement : {train_time_lstm:.3f} s | Temps prédiction : {pred_time_lstm:.3f} s\")\n",
        "print(f\"Micro: P={prec_micro_l:.3f} R={rec_micro_l:.3f} F1={f1_micro_l:.3f}\")\n",
        "print(f\"Macro: P={prec_macro_l:.3f} R={rec_macro_l:.3f} F1={f1_macro_l:.3f}\")\n",
        "\n",
        "print(\"\\nHistorique entraînement (dernière epoch) :\")\n",
        "print({k: float(v[-1]) for k, v in hist.history.items()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPtt7dPtCn70",
        "outputId": "8f489886-4873-4139-8860-c13da4ea1b03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Modèle (BiLSTM) ===\n",
            "Temps entraînement : 309.007 s | Temps prédiction : 1.489 s\n",
            "Micro: P=0.671 R=0.392 F1=0.495\n",
            "Macro: P=0.436 R=0.231 F1=0.290\n",
            "\n",
            "Historique entraînement (dernière epoch) :\n",
            "{'loss': 0.02974589169025421, 'val_loss': 0.2129724621772766}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Etape 3**"
      ],
      "metadata": {
        "id": "K-Msww98C519"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 1 — Export artefacts (.keras, tokenizer, labels, preprocess) ===\n",
        "import os, json, pathlib\n",
        "\n",
        "try:\n",
        "    model, tokenizer, label_cols\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Modèle/tokenizer/labels introuvables. Exécuter l'Étape 2 (Blocs 1–3).\")\n",
        "\n",
        "# Reprise du clean_text de l'étape 2 pour le module preprocess\n",
        "EMOJI_RE = re.compile(r\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
        "URL_RE   = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "def clean_text(s: str) -> str:\n",
        "    s = str(s).lower()\n",
        "    s = URL_RE.sub(\" \", s)\n",
        "    s = EMOJI_RE.sub(\" \", s)\n",
        "    s = re.sub(r\"[^a-z0-9\\s']\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "EXPORT_DIR = \"/content/lstm_service\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Modèle\n",
        "model_path = os.path.join(EXPORT_DIR, \"model.keras\")\n",
        "model.save(model_path)\n",
        "\n",
        "# 2) Tokenizer\n",
        "with open(os.path.join(EXPORT_DIR, \"tokenizer.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(tokenizer.to_json())\n",
        "\n",
        "# 3) Labels\n",
        "with open(os.path.join(EXPORT_DIR, \"labels.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for lab in label_cols:\n",
        "        f.write(lab + \"\\n\")\n",
        "\n",
        "# 4) Module preprocess\n",
        "with open(os.path.join(EXPORT_DIR, \"preprocess.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write('''import re\n",
        "EMOJI_RE = re.compile(r\"[\\\\U00010000-\\\\U0010ffff]\", flags=re.UNICODE)\n",
        "URL_RE   = re.compile(r\"https?://\\\\S+|www\\\\.\\\\S+\")\n",
        "def clean_text(s: str) -> str:\n",
        "    s = str(s).lower()\n",
        "    s = URL_RE.sub(\" \", s)\n",
        "    s = EMOJI_RE.sub(\" \", s)\n",
        "    s = re.sub(r\"[^a-z0-9\\\\s']\", \" \", s)\n",
        "    s = re.sub(r\"\\\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "''')\n",
        "\n",
        "print(\"Export terminé. Fichiers :\")\n",
        "print(\"\\n\".join([str(p) for p in pathlib.Path(EXPORT_DIR).glob('*')]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VSfeJNmD28B",
        "outputId": "d581e22d-dbae-4ab3-9add-dfd7df69a0f7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export terminé. Fichiers :\n",
            "/content/lstm_service/preprocess.py\n",
            "/content/lstm_service/model.keras\n",
            "/content/lstm_service/labels.txt\n",
            "/content/lstm_service/tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 2 — API FastAPI (pour .keras) ===\n",
        "import os\n",
        "from textwrap import dedent\n",
        "\n",
        "app_code = dedent(\"\"\"\n",
        "import os, json\n",
        "from typing import List, Dict\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "from preprocess import clean_text\n",
        "\n",
        "MAX_LEN = 120\n",
        "MODEL_PATH = \"model.keras\"\n",
        "\n",
        "with open(\"tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    tok_json = f.read()\n",
        "tokenizer = tokenizer_from_json(tok_json)\n",
        "\n",
        "with open(\"labels.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    LABELS = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "app = FastAPI(title=\"Toxic Comment LSTM API\", version=\"1.0\")\n",
        "\n",
        "class PredictIn(BaseModel):\n",
        "    texts: List[str]\n",
        "\n",
        "class PredictOut(BaseModel):\n",
        "    scores: List[Dict[str, float]]\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\", \"labels\": LABELS}\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictOut)\n",
        "def predict(payload: PredictIn):\n",
        "    cleaned = [clean_text(t) for t in payload.texts]\n",
        "    seqs = tokenizer.texts_to_sequences(cleaned)\n",
        "    pad = pad_sequences(seqs, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "    preds = model.predict(pad, verbose=0)\n",
        "    out = []\n",
        "    for row in preds.tolist():\n",
        "        out.append({LABELS[i]: float(row[i]) for i in range(len(LABELS))})\n",
        "    return PredictOut(scores=out)\n",
        "\"\"\")\n",
        "\n",
        "with open(os.path.join(EXPORT_DIR, \"app.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "reqs = \"\"\"fastapi==0.115.0\n",
        "uvicorn[standard]==0.30.6\n",
        "tensorflow==2.16.1\n",
        "\"\"\"\n",
        "with open(os.path.join(EXPORT_DIR, \"requirements.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(reqs)\n",
        "\n",
        "print(\"Fichiers API écrits : app.py, requirements.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjtffgvTEEFb",
        "outputId": "872e400a-1b74-41df-888c-9d34d9094a44"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichiers API écrits : app.py, requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 3 — Test offline (.keras) ===\n",
        "import json\n",
        "import numpy as np, tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open(os.path.join(EXPORT_DIR, \"tokenizer.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    tok_json = f.read()\n",
        "tok2 = tokenizer_from_json(tok_json)\n",
        "\n",
        "with open(os.path.join(EXPORT_DIR, \"labels.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    labels2 = [l.strip() for l in f if l.strip()]\n",
        "\n",
        "model2 = tf.keras.models.load_model(os.path.join(EXPORT_DIR, \"model.keras\"))\n",
        "\n",
        "# clean local aligné\n",
        "EMOJI_RE = re.compile(r\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
        "URL_RE   = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "def clean_text_local(s):\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(\" \", s)\n",
        "    s = EMOJI_RE.sub(\" \", s)\n",
        "    s = re.sub(r\"[^a-z0-9\\s']\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "samples = [\n",
        "    \"you are beauftiful\",\n",
        "    \"you are horrible\",\n",
        "]\n",
        "cleaned = [clean_text_local(t) for t in samples]\n",
        "seqs = tok2.texts_to_sequences(cleaned)\n",
        "pad = pad_sequences(seqs, maxlen=120, padding=\"post\", truncating=\"post\")\n",
        "preds = model2.predict(pad, verbose=0)\n",
        "\n",
        "print(\"Sortie prédite (scores par label) :\")\n",
        "for i, sc in enumerate(preds):\n",
        "    print(f\"- Ex{i+1}:\")\n",
        "    print({labels2[j]: float(sc[j]) for j in range(len(labels2))})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj2zkVqjEKqI",
        "outputId": "b8073be6-b6c4-4d07-b4b7-3e640ed6a28e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sortie prédite (scores par label) :\n",
            "- Ex1:\n",
            "{'toxic': 0.7940958738327026, 'severe_toxic': 0.003181144827976823, 'obscene': 0.08463543653488159, 'threat': 0.012225757353007793, 'insult': 0.05774860829114914, 'identity_hate': 0.008261489681899548}\n",
            "- Ex2:\n",
            "{'toxic': 0.8216884732246399, 'severe_toxic': 0.003536294214427471, 'obscene': 0.09045542031526566, 'threat': 0.01316509023308754, 'insult': 0.0632026344537735, 'identity_hate': 0.009455006569623947}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Bloc 4 — Dockerfile (pour déploiement) ===\n",
        "dockerfile = \"\"\"\n",
        "FROM python:3.10-slim\n",
        "\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends build-essential && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "WORKDIR /app\n",
        "COPY requirements.txt /app/requirements.txt\n",
        "RUN pip install --no-cache-dir -r /app/requirements.txt\n",
        "\n",
        "# Artefacts\n",
        "COPY model.keras /app/model.keras\n",
        "COPY tokenizer.json /app/tokenizer.json\n",
        "COPY labels.txt /app/labels.txt\n",
        "COPY preprocess.py /app/preprocess.py\n",
        "COPY app.py /app/app.py\n",
        "\n",
        "EXPOSE 8080\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
        "\"\"\"\n",
        "with open(os.path.join(EXPORT_DIR, \"Dockerfile\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(dockerfile)\n",
        "\n",
        "print(\"Dockerfile écrit :\", os.path.join(EXPORT_DIR, \"Dockerfile\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CVaxRsHERPg",
        "outputId": "d8e8945a-9848-4df2-fd95-25ae7d0b5e93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dockerfile écrit : /content/lstm_service/Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GitHub**"
      ],
      "metadata": {
        "id": "Nhs1H9eDYxCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab: installer git & git-lfs\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y git git-lfs\n",
        "!git lfs install\n",
        "\n",
        "# Config Git (mets ton nom et ton email GitHub)\n",
        "!git config --global user.name \"Willy772\"\n",
        "!git config --global user.email \"tallawilly@icloud.com\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUVqVpeGY1X8",
        "outputId": "da78e84b-0f3c-497d-decc-6c87ca444dfe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    }
  ]
}